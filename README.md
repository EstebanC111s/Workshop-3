# ğŸ“Š Workshop-3 â€” Happiness Score ETL + ML + Kafka + Postgres + Power BI

This repository contains the full solution for **Workshop-3**.

We implement an end-to-end pipeline that:

- Ingests **5 yearly CSV files** (World Happiness dataset).
- Performs **EDA + ETL** to create a unified, clean dataset.
- Trains an **interpretable regression model** to predict the *Happiness Score*.
- Streams features through **Apache Kafka** (Python Producer).
- Consumes messages, generates predictions, and loads them into a **PostgreSQL Data Warehouse** (Python Consumer).
- Connects **Power BI** directly to the DW to build **KPIs & visualizations** for model evaluation.

The implementation follows the block diagram:

- **Top box** â†’ CSVs â†’ EDA/ETL â†’ model training â†’ feature selection â†’ data streaming.
- **Bottom box** â†’ Kafka consumer â†’ model prediction â†’ database load â†’ analytics layer.

---

## ğŸ§  ETL & Modeling Flow

| âš™ï¸ Step                         | ğŸ” Description |
|---------------------------------|----------------|
| ğŸ“¥ **Extract**                  | Read 2015â€“2019 CSV files from `data/raw/` |
| ğŸ **Transform (EDA + ETL)**    | Standardize schemas, select relevant columns, handle basic quality checks, and concatenate into a unified dataset |
| ğŸ“¦ **Unified Dataset**         | Persist `unified.csv` in `data/processed/` as the single source of truth for training |
| ğŸ§® **Feature Selection**        | Choose socio-economic drivers + Country/Region as model inputs |
| ğŸ§ª **Model Training**           | Train a regression model (Ridge/Linear) to predict Happiness Score, export `model.pkl` + `feature_list.json` + `split_index.csv` |
| ğŸ“¡ **Kafka Producer (Features)**| Read & transform records, attach `row_id`, `is_train`, `is_test`, and send feature messages to Kafka topic `happiness_features` |
| ğŸ“¥ **Kafka Consumer**           | Subscribe to topic, rebuild features, load `model.pkl`, generate predictions `y_pred` |
| ğŸ—„ï¸ **DW Load (Postgres)**       | Insert results into `public.fact_predictions` (features + `y_true` + `y_pred` + flags) |
| ğŸ“Š **BI Layer (Power BI)**      | Connects to Postgres, builds KPIs (RÂ², RMSE, MAE, etc.) and dashboards on top of `fact_predictions` |

---

## ğŸ“ Project Structure

```bash
Workshop-3/
â”œâ”€ src/
â”‚  â”œâ”€ train_model.py          # EDA/ETL-to-training: unified dataset, split, model, artifacts
â”‚  â”œâ”€ producer_features.py    # Kafka producer: streams feature records from CSVs
â”‚  â””â”€ consumer_dw.py          # Kafka consumer: predicts and loads into Postgres DW
â”‚
â”œâ”€ notebooks/
â”‚  â””â”€ EDA_unified.ipynb       # (optional) Exploratory + ETL notebook used to design the pipeline
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â”‚  â”œâ”€ 2015.csv
â”‚  â”‚  â”œâ”€ 2016.csv
â”‚  â”‚  â”œâ”€ 2017.csv
â”‚  â”‚  â”œâ”€ 2018.csv
â”‚  â”‚  â””â”€ 2019.csv
â”‚  â””â”€ processed/
â”‚     â”œâ”€ unified.csv          # unified happiness dataset
â”‚     â””â”€ artifacts/
â”‚        â”œâ”€ model.pkl         # trained regression model (ignored in git)
â”‚        â”œâ”€ feature_list.json # expected feature order at serving time
â”‚        â””â”€ split_index.csv   # train/test assignment per row_id
â”‚
â”œâ”€ powerbi/
â”‚  â””â”€ Happiness_Dashboard.pbix # (optional) Power BI report connected to Postgres
â”‚
â”œâ”€ docker-compose.yml          # Kafka + Zookeeper + Postgres + Kafka UI
â”œâ”€ requirements.txt
â”œâ”€ .env                        # environment variables (not committed)
â”œâ”€ .gitignore
â””â”€ README.md
```
# âš™ï¸ Configuration
```
# Kafka
KAFKA_BROKER=localhost:9087
KAFKA_TOPIC=happiness_features

# Postgres DW
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=dw_happiness
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

# Paths
DATA_DIR=./data
RAW_DIR=./data/raw
PROCESSED_DIR=./data/processed

# Environment
ENV=dev

```

# ğŸ³ Run with Docker (Kafka + Postgres stack)

```
# Start Kafka, Zookeeper, Postgres, and Kafka UI
docker compose up -d
```
Typical services:

Kafka on localhost:9087

Kafka UI on http://localhost:<ui-port>

Postgres on localhost:5432 (dw_happiness DB)

# ğŸš€ End-to-End Pipeline
# 1ï¸âƒ£ Training phase

Run once (after placing CSVs in data/raw/):
```
# 1) Create unified dataset + train model + export artifacts
python -m src.train_model

```

# 2ï¸âƒ£ Streaming phase (Producer)

With Docker stack running:

```
python -m src.producer_features
```

# 3ï¸âƒ£ Serving + DW Load phase (Consumer)

In another terminal (stack running, model artifacts available):

```
python -m src.consumer_dw


```
